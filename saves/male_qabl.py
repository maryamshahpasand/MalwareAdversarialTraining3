import numpy as np
import sys
import psutil
import os
import csv
import random
from timeit import default_timer as timer
import pickle

import torch
import torch.nn as nn
# from torchvision import transforms
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data_utils

from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import linear_model, svm, tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import normalize
import matplotlib.pyplot as plt

import collections, functools, operator
import sklearn
import scipy.sparse
import pandas as pd




def cpuStats():
    print(sys.version)
    print(psutil.cpu_percent())
    print(psutil.virtual_memory())  # physical memory usage
    pid = os.getpid()
    py = psutil.Process(pid)
    memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think
    print('memory GB:', memoryUse)


class Generator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        # self.map2 = nn.Linear(hidden_size, hidden_size)
        # self.map3 = nn.Linear(hidden_size, output_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.sigmoid(self.map1(x))
        x = torch.sigmoid(self.map2(x))
        # x = F.sigmoid(self.map3(x))

        return torch.max(x_and_example[0], x)


class Discriminator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Discriminator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.sigmoid(self.map1(x))
        return torch.sigmoid(self.map2(x))


class GAN():
    def __init__(self, input_size, hidden_size, noise_size):
        self.otiginal_data_size = 0
        self.noise_size = noise_size
        self.gloss = []
        self.dloss = []
        self.criterion = nn.BCELoss()
        self.criterion_ = nn.L1Loss()
        self.D = Discriminator(input_size=input_size, hidden_size=hidden_size, output_size=1).cuda()
        self.d_optimizer = optim.Adam(self.D.parameters(), lr=0.001, betas=(0.9, 0.999))
        self.G = Generator(input_size=input_size + self.noise_size, hidden_size=hidden_size,
                           output_size=input_size).cuda()
        self.g_optimizer = optim.Adam(self.G.parameters(), lr=0.0001, betas=(0.9, 0.999))

    def plot_added_featues(self,list_of_added_features):
        frams = pd.DataFrame(list_of_added_features)
        ax = frams.plot.bar(stacked=True)
        ax.set_xlabel('EPOCH')
        # for rowNum, row in frams.iterrows():
        #     ypos = 0
        #     featuer=0
        #     for val in row:
        #         if featuer!=6:
        #             ypos += val
        #             ax.text(rowNum, ypos , "{0:.2f}".format(val), color='black' ,ha='center')
        #         featuer+=1
        #     ypos = 0
        plt.title('Average Number of Added Features in Each Epoch')
        # chartBox = ax.get_position()
        # ax.set_position([chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])
        # ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.8), ncol=1)
        plt.show()

    def process_batch(self , local_batch , local_lable):

        xmal_batch = local_batch[(local_lable != 0).nonzero()]
        # print(xmal_batch.shape)

        if (len(xmal_batch.shape) > 2):
            xmal_batch = xmal_batch.reshape((xmal_batch.shape[0] * xmal_batch.shape[1]), xmal_batch.shape[2])
            # print(xmal_batch.shape)

        xben_batch = local_batch[(local_lable == 0).nonzero()]
        # print(xben_batch.shape)
        if len(xben_batch.shape) > 2:
            xben_batch = xben_batch.reshape((xben_batch.shape[0] * xben_batch.shape[1]), xben_batch.shape[2])
        # print(xben_batch.shape)

        return xmal_batch , xben_batch

    def check_added_features(self, x, features):
        added_features = []
        for i in range(len(x)):
            unique, counts = np.unique(
                np.where(x.cpu().numpy()[i].astype(int) > 0, features[:, 2], np.zeros(features[:, 2].shape)),
                return_counts=True)
            unique = unique[1:]
            counts = counts[1:]
            added_features.append(dict(zip(unique, counts)))

        batch_added_features = dict(functools.reduce(operator.add, map(collections.Counter, added_features)))
        batch_added_features.update({n: batch_added_features[n] / len(x) for n in batch_added_features.keys()})
        return batch_added_features

    def train(self, _classifier, epochs, data, batch_size=32, round=-1):
        # Load and Split the dataset

        # (x_org_mal, y_org_mal)= data[0][0][:data_size[0]], data[0][1][:data_size[0]]
        # (x_org_ben, y_org_ben) = data[1][0][:data_size[0]], data[1][1][:data_size[0]],
        # (xmal, ymal), (xben, yben) =  data[0], data[1]
        #
        # X_org = np.concatenate([x_org_mal, x_org_ben])
        # Y_org = np.concatenate([y_org_mal, y_org_ben])
        #
        # X_org_adv = np.concatenate([xben, xmal])
        # Y_org_adv = np.concatenate([yben, ymal])
        # xtest, ytest =  data[2][0], data[2][1]


        features = np.load('/home/maryam/Code/data/Drebin_processed_features/randomforest_important_features/0.001/Drebin_important_features.npy')
        features=np.append(features,[features[len(features)-1]], axis=0)
        (xmal, ymal), (xben, yben) = data[0], data[1]
        X = np.concatenate([xben, xmal])
        Y = np.concatenate([yben, ymal])
        xtrain, ytrain, xtest, ytest = X, Y, data[2][0], data[2][1]

        #sampling for unbalanced data
        class_sample_count = np.array(
            [len(np.where(ytrain == t)[0]) for t in np.unique(ytrain)])
        weight = 1. / class_sample_count
        samples_weight = []
        for t in range(len(ytrain) - 1):
            samples_weight.append(weight[int(ytrain[t])])
        sampler = data_utils.sampler.WeightedRandomSampler(samples_weight, len(samples_weight))
        train = data_utils.TensorDataset(torch.from_numpy(xtrain), torch.from_numpy(ytrain))
        train_loader = data_utils.DataLoader(train, batch_size=batch_size, sampler=sampler)

        print ('\nTRAINING GAN...\n')
        start_train = timer()
        list_of_added_features = []

        Train_FNR, Test_FNR = [], []
        best_test_FNR, best_train_FNR = 0.0, 0.0
        self.gloss, self.dloss = [], []
        for epoch in range(epochs):

            batch_added_features=[]
            for local_batch, local_lable in train_loader:
                # ---------------------
                #  Train substitute_detector
                # ---------------------

                xmal_batch ,xben_batch = self.process_batch(local_batch , local_lable)
                if xmal_batch.shape[0]<1 or xben_batch.shape[0]<1:
                    break
                else:


                    noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                    yclassifierben_batch = _classifier.predict(xben_batch)

                    # Generate a batch of new malware examples
                    gen_examples = self.G([xmal_batch.float().cuda(), noise.float().cuda()]).detach()
                    batch_added_features.append(
                        self.check_added_features((torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float()) - xmal_batch.cuda().float(), features))
                    yganmal_batch = _classifier.predict(
                        np.ones(gen_examples.cpu().detach().numpy().shape) * (gen_examples.cpu().detach().numpy() > 0.5))

                    # Train the substitute_detector
                    d_fake_decision = self.D(torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float())
                    d_loss_fake = self.criterion(d_fake_decision.squeeze(), torch.from_numpy(yganmal_batch).float().cuda())
                    d_real_decision = self.D(xben_batch.float().cuda())
                    d_loss_real = self.criterion(d_real_decision.squeeze(), torch.from_numpy(yclassifierben_batch).float().cuda())
                    d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)

                    self.D.zero_grad()
                    d_loss.backward(retain_graph=True)
                    self.d_optimizer.step()

                    # ---------------------
                    #  Train Generator
                    # ---------------------

                    noise = torch.rand(xmal_batch.shape[0], self.noise_size)

                    # Train the generator
                    g_fake_data = self.G([xmal_batch.float().cuda(), noise.float().cuda()])
                    dg_fake_decision = self.D(g_fake_data)
                    g_loss_samples = self.criterion(dg_fake_decision.squeeze(), torch.zeros(xmal_batch.shape[0]).cuda())
                    # orig_adv_dist = np.diag(
                    #     sklearn.metrics.pairwise.manhattan_distances(torch.round(g_fake_data).cpu().detach().numpy(),
                    #                                                  xmal_batch))
                    # g_loss_distance = self.criterion_(torch.from_numpy((normalize(orig_adv_dist.reshape(-1, 1))).ravel()).float().cuda(), torch.zeros(orig_adv_dist.shape[0]).cuda())
                    # g_loss_distance = self.criterion_(torch.from_numpy(orig_adv_dist).float().cuda(),
                    #                                   torch.zeros(orig_adv_dist.shape[0]).cuda())
                    g_loss = g_loss_samples
                    self.G.zero_grad()
                    g_loss.backward()
                    self.g_optimizer.step()

                    torch.cuda.empty_cache()

            self.gloss.append(g_loss)
            self.dloss.append(d_loss)
            list_of_added_features.append(pd.DataFrame(batch_added_features).mean(axis=0).to_dict())
            # Compute attach success rate on train data
            xtrain_mal = xtrain[(ytrain == 1).nonzero()[0]]
            noise = np.random.uniform(0, 1, (xtrain_mal.shape[0], self.noise_size))
            gen_examples = self.G(
                [torch.from_numpy(xtrain_mal).float().cuda(), torch.from_numpy(noise).float().cuda()])
            gen_examples = np.ones(gen_examples.cpu().detach().numpy().shape) *(gen_examples.cpu().detach().numpy() > 0.5)
            if (sum(np.ones(gen_examples.shape[0])- classifier_.predict(gen_examples)) != 0):
                conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]), classifier_.predict(gen_examples))
                train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
            else:
                train_FNR = 0

            if train_FNR > best_train_FNR:
                best_train_FNR = train_FNR
            Train_FNR.append(train_FNR)
            # Compute attach success rate on test data
            xtest_mal = xtest[(ytest != 0).nonzero()[0]]
            noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.noise_size))
            gen_examples = self.G(
                [torch.from_numpy(xtest_mal).float().cuda(), torch.from_numpy(noise).float().cuda()])
            gen_examples = np.ones(gen_examples.cpu().detach().numpy().shape) *(gen_examples.cpu().detach().numpy() > 0.5)

            if (sum(np.ones(gen_examples.shape[0])- classifier_.predict(gen_examples)) != 0):
                conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]), classifier_.predict(gen_examples))
                test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
            else:
                test_FNR = 0
            Test_FNR.append(test_FNR)
            if train_FNR > best_train_FNR:
                best_train_FNR = train_FNR
            if test_FNR > best_test_FNR:
                best_test_FNR = test_FNR
                print('saving mulgan weights at epoch:', epoch)

                print("[D loss: %f] [G loss: %f] \n" % (self.dloss[-1], self.gloss[-1]))
                torch.save(self.G,
                           '/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/malgan{0}.pt'.format(round))

            # Print the progress
            # if epoch % 20 == 0:
            print(pd.DataFrame(batch_added_features).mean(axis=0))
            print("%d [D loss: %f] [G loss: %f] [test_FNR: %f] [train_FNR: %f] [disstortion: %f]" % (epoch, d_loss, g_loss , test_FNR,train_FNR , pd.DataFrame(batch_added_features).mean(axis=0).sum()))
        end_train = timer()
        del g_loss, d_loss

        print('\ntraining completed in %.3f seconds.\n' % (end_train - start_train))
        print('=============results ============= ')

        print(' attack success rate using train data: {0} \n'
              ' attack success rate using test data: {1}'.format(best_train_FNR, best_test_FNR))
        print('==============================\n ')

        self.plot_added_featues(list_of_added_features)

        # Plot losses
        plt.figure()
        plt.plot(range(len(self.gloss)), self.gloss, c='r', label='g_loss_rec', linewidth=2)
        plt.plot(range(len(self.dloss)), self.dloss, c='g', linestyle='--', label='d_loss', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('loss')
        plt.legend()
        plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/GAN_Epoch_loss({0}).png'.format(round))
        # plt.show()
        plt.close()

        # Plot TPR
        plt.figure()
        plt.plot(range(len(Train_FNR)), Train_FNR, c='r', label='Training Set', linewidth=2)
        plt.plot(range(len(Test_FNR)), Test_FNR, c='g', linestyle='--', label='Validation Set', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('FNR')
        plt.legend()
        plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/Epoch_FNR({0}).png'.format(round))
        # plt.show()
        plt.close()



        return [best_train_FNR, best_test_FNR]


def build_classifier(type='RF', data=[]):
    if type is 'RF':
        model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=1)
    elif type is 'SVM':
        model = svm.SVC()
    elif type is 'RBF_SVM':
        # Parameters={'kernel':['rbf'],'gamma': [1e-3, 1e-4],'C': [0.001, 0.01, 0.1]}
        # model = GridSearchCV(svm.SVC(), Parameters, cv=5, scoring='f1', n_jobs=-1)
        model = svm.SVC(kernel='rbf')
    elif type is 'LR':
        model = linear_model.LogisticRegression()
    elif type is 'DT':
        model = tree.DecisionTreeRegressor()
    elif type is 'MLP':
        model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
                              solver='sgd', verbose=0, tol=1e-4, random_state=1,
                              learning_rate_init=.1)

    (xmal, ymal), (xben, yben), (xtsmal, xtsben), (ytsmal, ytsben) = data[0], data[1], data[2], data[3]
    # xtrain = scipy.sparse.vstack((xben, xmal))
    # ytrain = np.concatenate([yben, ymal])
    # xtest = scipy.sparse.vstack((xtsmal, xtsben))
    # ytest = np.concatenate([ytsmal, ytsben])
    xtrain = np.concatenate([xben, xmal])
    ytrain = np.concatenate([yben, ymal])
    xtest = np.concatenate([xtsmal, xtsben])
    ytest = np.concatenate([ytsmal, ytsben])

    print('\n\n--- Round: {0} '.format(0))

    print('=============data============= ')
    print(
        'number of original malware = {0}\n'
        'number of adversarial samples (old+new) = {1}\n'
        'number of new adversarial samples = {2}\n'
        'Malware to benign Ratio: {3:.2f}\n'
        'adversarial samples portion of malware samples = {4:.2f}\n'
        'adversarial samples portion of all samples = {5:.2f} '
            # .format(len(xmal) + len(data[2][0][(data[2][1]==1).nonzero()[0]]),
            .format(xmal.shape[0],
                    0,
                    0,
                    xmal.shape[0] / xben.shape[0],
                    0,
                    0
                    ))
    print('==============================\n ')


    print ('TRAINING CLASSIFIER...\n')
    start_train = timer()
    if os.path.isfile('./models/male_qabl/' + type + '__' + str(por) + '.sav'):
        # if os.path.isfile('/home/maryam/Code/python/adversarial_training/torch_impl/Different_ratio/models/'+type+'weighted__'+por+'.sav'):
        model = pickle.load(open('./modelsmale_qabl/' + type + '__' + str(por) + '.sav', 'rb'))
        # '/home/maryam/Code/python/adversarial_training/torch_impl/Different_ratio/models/'+type+'weighted__'+por+'.sav', 'rb'))
    else:
        model.fit(xtrain, ytrain)
        pickle.dump(model, open('./models/male_qabl/' + type + '__' + str(por) + '.sav', 'wb'))
    end_train = timer()

    print('training completed in %.3f seconds:' % (end_train - start_train))
    print('=============results ============= ')

    conf_matx = confusion_matrix(ytrain, model.predict(xtrain))
    print('Train Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3}'.format(conf_matx[0][0], conf_matx[0][1], conf_matx[1][0],
                                                           conf_matx[1][1]))
    train_FNR =  conf_matx[1][0]/(conf_matx[1][0]+conf_matx[1][1])

    conf_matx = confusion_matrix(ytest, model.predict(xtest))
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1], conf_matx[1][0],
                                                           conf_matx[1][1]))
    test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    Original_Train_TPR = model.score(xtrain, ytrain)
    Original_Test_TPR = model.score(xtest, ytest)
    print('Original_Train_TPR: {0}, Original_Test_TPR: {1} , FNR for train: {2} , FNR for test: {3}'. format(Original_Train_TPR , Original_Test_TPR , train_FNR ,test_FNR ) )
    print('==============================\n ')

    return model , [Original_Train_TPR , Original_Test_TPR , Original_Test_TPR , train_FNR ,test_FNR, test_FNR]


def retrain_classifier(classifier, _GAN, data, round):
    print('\n\n--- Round: {0} '.format(round+1))

    print('\n\nADVESARIAL RETRAINING...\n\n')

    (xmal, ymal), (xben, yben), xtest, ytest = data[0], data[1], data[2][0], data[2][1]
    xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)
    # Generate Train Adversarial Examples
    print('Generating Train Adversarial Examples...\n')
    noise = np.random.uniform(0, 1, (data_size[0], _GAN.noise_size))
    _GAN.G = torch.load('/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/malgan{0}.pt'.format(round))
    with torch.no_grad():
        # not to generate adv version from other avd samples
        gen_examples = _GAN.G([torch.from_numpy(data[0][0][:data_size[0]]).float().cuda(),
                               torch.from_numpy(noise).float().cuda()]).detach()
    x_gen_examples = (torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float()).cpu().numpy()
    y_gen_examples = np.ones(len(gen_examples.cpu()))
    train_gen_examples, test_gen_examples, y_train_gen_examples, y_test_gen_examples = train_test_split(x_gen_examples,
                                                                                                        y_gen_examples,
                                                                                                        test_size=0.20)
    data_size.append(len(gen_examples))
    print('==============new data============ ')
    print(
        'number of original malware = {3}\n'
        'number of adversarial samples (old+new) = {4}\n'
        'number of new adversarial samples = {0}\n'
        'Malware to benign Ratio: {5:.2f}\n'
        'adversarial samples portion of malware samples = {1:.2f}\n'
        'adversarial samples portion of all samples = {2:.2f} '
            .format(len(gen_examples),
                    (np.sum(data_size) - data_size[0]) / np.sum(data_size),
                    (np.sum(data_size) - data_size[0]) / (np.sum(data_size) + len(data[1][0])),
                    data_size[0],
                    np.sum(data_size) - data_size[0],
                    len(data[0][0]) / len(data[1][0])
                    ))
    print('==================================\n ')
    # #select the closest adv samples to original samples to add to training set
    # orig_adv_dist = np.diag(sklearn.metrics.pairwise.euclidean_distances(gen_examples, data[0][0][:data_size[0]]))
    # orig_adv_dist_train = np.diag(sklearn.metrics.pairwise.manhattan_distances(gen_examples, data[0][0][:data_size[0]]))
    # # select best 20 percent of adversarial samples to add to training set and teach the classifier
    # # portion_to_add_to_train_data = np.int(0.2 * len(orig_adv_dist))
    # portion_to_add_to_train_data = np.int(1.0 * len(orig_adv_dist))
    # indext_of_best_adv_samples = np.argpartition(orig_adv_dist, portion_to_add_to_train_data)[:portion_to_add_to_train_data]
    # adv_to_add_to_train = gen_examples[indext_of_best_adv_samples]

    print('retraining classifier...\n')
    start_train = timer()

    classifier.fit(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples]),
                   np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    end_train = timer()
    print('training completed in %.3f seconds:' % (end_train - start_train))
    print('\n=============results ============= ')

    # Compute Train TPR
    train_TPR = classifier.score(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples]),
                                 np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    conf_matx = confusion_matrix(classifier.predict(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples])),
                                 np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    print('Train Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3}'.format(conf_matx[0][0], conf_matx[0][1],
                                                                         conf_matx[1][0],
                                                                         conf_matx[1][1]))
    train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    # Compute Test
    test_TPR = classifier.score(np.concatenate([xtest_mal, xtest_ben, test_gen_examples]),
                                np.concatenate([ytest_mal, ytest_ben, y_test_gen_examples]))

    conf_matx = confusion_matrix(classifier.predict(np.concatenate([xtest_mal, xtest_ben, test_gen_examples])),
                                 np.concatenate([ytest_mal, ytest_ben, y_test_gen_examples]))
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                          conf_matx[1][0],
                                                                          conf_matx[1][1]))
    test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    Clean_test_TPR = classifier.score(xtest, ytest)

    conf_matx = confusion_matrix(classifier.predict(xtest), ytest)
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                          conf_matx[1][0],
                                                                          conf_matx[1][1]))
    Clean_test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    print(
        'Train accuracy: {0}\n Test accuracy: {1}  \n FNR for train: {2} \n FNR for test: {3} \n Clean_test_TPR: {4} \n Clean_test_FNR: {5}  '
            .format(train_TPR, test_TPR, train_FNR, test_FNR, Clean_test_TPR, Clean_test_FNR))
    print('==============================\n ')

    # keeping the generated samples from previous rounds
    data[0] = (np.concatenate([data[0][0], gen_examples]), np.concatenate([data[0][1], np.ones(len(gen_examples))]))

    # plot adv and orig samples distance
    orig_adv_dist = np.diag(sklearn.metrics.pairwise.manhattan_distances(gen_examples, data[0][0][:data_size[0]]))

    plot_dist.append(orig_adv_dist[range(len(gen_examples) - 1)])

    plt.figure()
    plt.boxplot(plot_dist, labels=[i for i in range(len(plot_dist))])

    # plt.show()
    plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/new_adv_samples_distortion.png')

    plt.xlabel('round of retraining(54 adv. sample in each round is added to dataset)')
    plt.ylabel('distance from original version')
    # plt.legend()
    # plt.show()
    # print(orig_adv_dist)

    return [train_TPR , test_TPR , Clean_test_TPR , train_FNR ,test_FNR, Clean_test_FNR]


def load_data():
    xtrain = scipy.sparse.load_npz(
        '/home/maryam/Code/data/Drebin_processed_features/randomforest_important_features/0.001/x_train.npz').toarray()
    ytrain = np.load(
        '/home/maryam/Code/data/Drebin_processed_features/randomforest_important_features/0.001/y_train.npy')
    xtrain_mal = xtrain[np.where(ytrain == 1)]
    xtrain_ben = xtrain[np.where(ytrain == 0)]
    ytrain_mal = ytrain[np.where(ytrain == 1)]
    ytrain_ben = ytrain[np.where(ytrain == 0)]

    xtest = scipy.sparse.load_npz(
        '/home/maryam/Code/data/Drebin_processed_features/randomforest_important_features/0.001/x_test.npz').toarray()
    ytest = np.load('/home/maryam/Code/data/Drebin_processed_features/randomforest_important_features/0.001/y_test.npy')
    features = np.load(
        '/home/maryam/Code/data/Drebin_processed_features/randomforest_important_features/0.001/Drebin_important_features.npy')
    features = np.append(features, [features[len(features) - 1]], axis=0)

    xtest_mal = xtest[np.where(ytest == 1)]
    xtest_ben = xtest[np.where(ytest == 0)]
    ytest_mal = ytest[np.where(ytest == 1)]
    ytest_ben = ytest[np.where(ytest == 0)]

    # # data = np.load('/home/maryam/Code/python/Malware-GAN-master/data.npz')
    # xben = np.load(
    #     '/home/maryam/Code/python/poisoning_project/datasets/Drebin_Kuafadet/np/x_benign_binary_features.npy')
    # xmal = np.load(
    #     '/home/maryam/Code/python/poisoning_project/datasets/Drebin_Kuafadet/np/x_malware_binary_features.npy')
    # # xmal = np.load('/home/maryam/Code/python/poisoning_project/datasets/Drebin_binary_features_11309all_11176unique/x_malware_binary_features.npy')
    # # xben = np.load('/home/maryam/Code/python/poisoning_project/datasets/Drebin_binary_features_11309all_11176unique/x_benign_binary_features.npy')
    #
    # # xmal = xmal[0:200]
    # ymal = np.ones(len(xmal))
    #
    # xben = xben[0:len(xmal)]
    # # xben = xben[0:len(xmal)+1000]
    # yben = np.zeros(len(xben))
    # # xmal, ymal, xben, yben = data['xmal'], data['ymal'], data['xben'], data['yben']
    # # ymal, yben = np.ones(len(xmal)) , np.zeros(len(xben))
    # # same number of malware and bengin samples
    # # return [(xmal[0,200], ymal[0,200]), (xben[0,200], yben[0,200])]
    # # xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    # xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    # xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)
    # # return [(xmal, ymal), (xben[0:len(xmal)], yben[0:len(xmal)])]
    # print(
    #     'number of malware for training = {0}\n'
    #     'number of benign for training= {1}\n'
    #     'number of malware for test  = {2}\n'
    #     'number of benign for test= {3}\n'
    #         .format(len(xtrain_mal),
    #                 len(xtrain_ben),
    #                 len(xtest_mal),
    #                 len(xtest_ben)))
    return [(xtrain_mal, ytrain_mal), (xtrain_ben, ytrain_ben),
            (xtest_mal, xtest_ben), (ytest_mal, ytest_ben)]
    # return [(xmal[0:round(len(xmal)/5)], ymal[0:round(len(ymal)/5)]), (xben, yben)]


if __name__ == '__main__':
    results = []
    data_all = load_data()
    # data_all= load_data.load_data(loaded= True)
    mal_num = data_all[0][0].shape[0]
    ben_num = data_all[1][0].shape[0]
    hist = {}
    hist['0.1'] = []
    hist['0.2'] = []
    hist['0.3'] = []
    hist['0.4'] = []
    hist['0.5'] = []
    por= 0.5 #, 0.4 , 0.3 , 0.2 ,0.1  ]:
    if por>0:
        porr = int(mal_num / por - mal_num)
    else:
        porr = ben_num-1
    print(
        'number of malware for training = {0}\n'
        'number of benign for training= {1}\n'
        'Malware portion  = {2}\n'
            .format(mal_num,
                    porr, por ))

    data_original = [#(xmal, ymal)
            data_all[0] ,
            #(xben,yben )
            (data_all[1][0][0:porr] ,data_all[1][1][0:porr]),
            #(xtsmal, xtsben)
            (data_all[2][0],data_all[2][1][0:int((data_all[1][0][0:porr] .shape[0])*(0.3/0.7))]) ,#portion of train to test
            # (ytsmal, ytsben)
            (data_all[3][0] , data_all[3][1][0:int((data_all[1][0][0:porr] .shape[0])*(0.3/0.7))]) ]#portion of train to test
    data = [#(xmal, ymal)
                data_all[0] ,
                #(xben,yben )
                (data_all[1][0][0:porr] ,data_all[1][1][0:porr]),
                #(xtsmal, xtsben)
                (data_all[2][0],data_all[2][1][0:int((data_all[1][0][0:porr] .shape[0])*(0.3/0.7))]) ,#portion of train to test
                # (ytsmal, ytsben)
                (data_all[3][0] , data_all[3][1][0:int((data_all[1][0][0:porr] .shape[0])*(0.3/0.7))]) ]#portion of train to test



    plot_dist = []
    GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=20)
    data_size = [len(data_original[0][0])]
    classifier_ , result_cls= build_classifier(type='RBF_SVM', data=data_original)
    result_gan = GAN_.train(classifier_, epochs=2, data=data_original, batch_size=100, round=0)
    results.append(np.concatenate([[0] , result_cls , result_gan]))
    for round in range(10):
        result_cls = retrain_classifier(classifier_, GAN_, data, round=round)
        del GAN_
        GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=20)
        result_gan = GAN_.train(classifier_, epochs=50, data=data_original, batch_size=100, round=round+1)
        results.append(np.concatenate([[round+1] ,result_cls, result_gan]))

    Header = ['Round', 'Train_accuray', 'Test_accuray', 'Test_validationset_accuray' , 'Train_FNR', 'Test_FNR', 'Test_validationset_FNR' , 'MCR on train data' , 'MCR on test data' ]
    print("Writing data ...")
    with open('/home/maryam/Code/python/adversarial_training/torch_impl/results.csv', 'w', newline='') as outfile1:
        wr = csv.writer(outfile1, delimiter=',', quoting=csv.QUOTE_NONE)
        wr.writerow([h for h in Header])
        for i in range(len(results)):
            wr.writerow(results[i])
        outfile1.close()

